image_finetune: true

output_dir: "outputs/"
folder_name: "v7"
pretrained_model_path: "./checkpoints/stable-diffusion-v1-5"
clip_model_path: "./checkpoints/clip-vit-base-patch32"
poseguider_checkpoint_path: "./checkpoints/v6/poseguider_stage_1.ckpt"
referencenet_checkpoint_path: "./checkpoints/v6/referencenet_stage_1.ckpt"
pretrained_unet_path: "./checkpoints/v6/unet_stage_1.ckpt"

noise_scheduler_kwargs:
  num_train_timesteps: 1000
  beta_start:          0.00085
  beta_end:            0.012
  # beta_schedule:       "scaled_linear"
  beta_schedule:       "linear"
  steps_offset:        1
  clip_sample:         false

description: "### Train Info: train stage 1: image pretrain ###"

unet_additional_kwargs:
  use_motion_module              : true
  motion_module_resolutions      : [ 1,2,4,8 ]
  unet_use_cross_frame_attention : false
  unet_use_temporal_attention    : false

  motion_module_type: Vanilla
  motion_module_kwargs:
    num_attention_heads                : 8
    num_transformer_block              : 1
    attention_block_types              : [ "Temporal_Self", "Temporal_Self" ]
    temporal_position_encoding         : true
    temporal_position_encoding_max_len : 24
    temporal_attention_dim_div         : 1
    zero_initialize                    : true


train_data:
  # csv_path:     "./data/UBC_train_info_test.csv"
  csv_path:     "./data/UBC_train_info.csv"
  video_folder: "/home/ubuntu/data/ubc_fashion/"
  sample_size:  768 # for 40G 256
  sample_stride: 4
  sample_n_frames: 16
  clip_model_path: "./checkpoints/clip-vit-base-patch32"


validation_data:
  prompts:
    - "Snow rocky mountains peaks canyon. Snow blanketed rocky mountains surround and shadow deep canyons."
    - "A drone view of celebration with Christma tree and fireworks, starry sky - background."
    - "Robot dancing in times square."
    - "Pacific coast, carmel by the sea ocean and waves."
  num_inference_steps: 
  guidance_scale: 8.

trainable_modules:
  # - "motion_modules."
  - "."

unet_checkpoint_path: ""

fusion_blocks: "full"

learning_rate:    1.e-5
train_batch_size: 4

max_train_epoch:      -1
max_train_steps:      200000
checkpointing_epochs: -1
checkpointing_steps:  5000
gradient_accumulation_steps: 1


validation_steps:       500000
validation_steps_tuple: [2, 50]

global_seed: 42
# mixed_precision_training: true
mixed_precision_training: False
# enable_xformers_memory_efficient_attention: True
enable_xformers_memory_efficient_attention: False


is_debug: False

# Steps:  14%|███████████████                                                                                               | 8212/60000 [12:32:06<77:11:34
# Steps:  28%|██████████████████████████████▎                                                                              | 16713/60000 [25:30:29<64:45:23,  5.39s/it, lr=1e-5, step_loss=0.0222]^
# Steps:   9%|█████████████▌                                                                                                 | 5515/60000 [8:24:46<81:21:36,  5.38s/it, lr=1e-5, step_loss=0.0227]
# Gradient accumulation of 1 with batch size of 4 = 
# Total iterations = 4 * (8212+16713+5515) = 121760
# Total required iterations = 30000 x 64 = 1920000
# Remaining iterations = 1920000 - 121760 = 1798240 = 28000 iterations with batch size 64